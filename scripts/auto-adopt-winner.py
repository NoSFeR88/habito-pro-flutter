#!/usr/bin/env python3
"""
A/B Testing - Auto-Adopt Winner
Automatically adopts winning variant when statistically significant.

Usage:
    python scripts/auto-adopt-winner.py --experiment exp-001-workflow-templates --dry-run
    python scripts/auto-adopt-winner.py --experiment exp-001-workflow-templates --execute
"""

import json
import argparse
import sys
from pathlib import Path
from datetime import datetime

# Paths
EXPERIMENTS_FILE = Path("telemetry/experiments/experiments.json")
REPORTS_DIR = Path("telemetry/experiments/reports")
ADOPTIONS_LOG = Path("telemetry/experiments/adoptions.json")


def load_experiments():
    """Load experiments configuration."""
    with open(EXPERIMENTS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_latest_analysis(experiment_id):
    """Load latest analysis report for an experiment."""
    if not REPORTS_DIR.exists():
        print(f"Error: No reports found in {REPORTS_DIR}")
        sys.exit(1)

    # Find latest report
    reports = list(REPORTS_DIR.glob(f"{experiment_id}_analysis_*.json"))

    if not reports:
        print(f"Error: No analysis report found for '{experiment_id}'")
        print("Run: python scripts/analyze-experiment.py --experiment {experiment_id} --export-report")
        sys.exit(1)

    latest_report = sorted(reports)[-1]

    with open(latest_report, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_adoptions_log():
    """Load adoptions log."""
    if not ADOPTIONS_LOG.exists():
        return {"adoptions": []}

    with open(ADOPTIONS_LOG, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_adoptions_log(data):
    """Save adoptions log."""
    ADOPTIONS_LOG.parent.mkdir(parents=True, exist_ok=True)
    with open(ADOPTIONS_LOG, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2)


def determine_winner(analysis):
    """Determine overall winner from analysis."""
    # Count wins per variant
    wins = {}
    for metric in analysis['metrics']:
        winner = metric['winner']
        wins[winner] = wins.get(winner, 0) + 1

    if not wins:
        return None

    # Return variant with most wins
    return max(wins, key=wins.get)


def create_adoption_documentation(experiment_id, analysis, winner_id):
    """Create documentation for the adoption."""
    experiments_data = load_experiments()
    exp = next((e for e in experiments_data['experiments'] if e['experiment_id'] == experiment_id), None)

    doc = f"""# AUTO-ADOPTION: {experiment_id}

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Experiment**: {exp['name']}
**Winner**: {winner_id}

## Summary

After analyzing {analysis['sample_size']} samples, the following variant has been automatically adopted:

**Winning Variant**: {winner_id}

## Statistical Evidence

"""

    for metric in analysis['metrics']:
        if metric['winner'] == winner_id:
            doc += f"### {metric['metric_name']}\n"
            doc += f"- Improvement: {metric['improvement_percent']}%\n"
            doc += f"- Effect Size: {metric['cohens_d']} ({metric['effect_size']})\n"
            doc += f"- Significance: p = {metric['p_value']}\n\n"

    doc += f"""## Actions Taken

1. Update default configuration to use '{winner_id}' variant
2. Document winning approach in best practices
3. Archive experiment results
4. Continue monitoring performance

## Recommendation

Based on statistical analysis, all future tasks should follow the '{winner_id}' approach
as documented in the experiment constraints.

---
*Auto-generated by auto-adopt-winner.py*
"""

    return doc


def adopt_winner(experiment_id, winner_id, analysis, dry_run=True):
    """Adopt winning variant."""
    experiments_data = load_experiments()
    exp = next((e for e in experiments_data['experiments'] if e['experiment_id'] == experiment_id), None)

    print(f"\n{'='*80}")
    print(f"AUTO-ADOPTION PROCESS")
    print(f"{'='*80}\n")
    print(f"Experiment: {exp['name']}")
    print(f"Winner: {winner_id}")
    print(f"Sample Size: {analysis['sample_size']}")

    # Get winner variant details
    winner_variant = next((v for v in exp['variants'] if v['id'] == winner_id), None)

    print(f"\nWinning Variant Details:")
    print(f"  Name: {winner_variant['name']}")
    print(f"  Description: {winner_variant['description']}")
    print(f"\nConstraints to adopt:")
    for key, value in winner_variant.get('constraints', {}).items():
        constraint_text = key.replace('_', ' ').title()
        print(f"  - {constraint_text}: {value}")

    print(f"\nStatistical Evidence:")
    for metric in analysis['metrics']:
        if metric['significant']:
            print(f"  - {metric['metric_name']}: {metric['improvement_percent']}% improvement (p={metric['p_value']})")

    if dry_run:
        print(f"\n{'='*80}")
        print(f"DRY RUN MODE - No changes made")
        print(f"{'='*80}\n")
        print("To execute adoption, run with --execute flag")
        return

    # Execute adoption
    print(f"\n{'='*80}")
    print(f"EXECUTING ADOPTION")
    print(f"{'='*80}\n")

    # 1. Create adoption documentation
    doc_content = create_adoption_documentation(experiment_id, analysis, winner_id)
    doc_file = Path(f"docs/adoptions/{experiment_id}_adoption.md")
    doc_file.parent.mkdir(parents=True, exist_ok=True)

    with open(doc_file, 'w', encoding='utf-8') as f:
        f.write(doc_content)

    print(f"1. Documentation created: {doc_file}")

    # 2. Update experiment status
    exp['status'] = 'adopted'
    exp['adopted_variant'] = winner_id
    exp['adopted_at'] = datetime.now().isoformat()

    with open(EXPERIMENTS_FILE, 'w', encoding='utf-8') as f:
        json.dump(experiments_data, f, indent=2)

    print(f"2. Experiment status updated to 'adopted'")

    # 3. Log adoption
    adoptions = load_adoptions_log()
    adoptions['adoptions'].append({
        "experiment_id": experiment_id,
        "winner_id": winner_id,
        "adopted_at": datetime.now().isoformat(),
        "sample_size": analysis['sample_size'],
        "metrics": analysis['metrics']
    })
    save_adoptions_log(adoptions)

    print(f"3. Adoption logged to: {ADOPTIONS_LOG}")

    print(f"\n{'='*80}")
    print(f"ADOPTION COMPLETE")
    print(f"{'='*80}\n")
    print(f"Next Steps:")
    print(f"1. Review documentation: {doc_file}")
    print(f"2. Update workflow templates with winning approach")
    print(f"3. Communicate changes to team")
    print(f"4. Monitor continued performance")
    print()


def main():
    parser = argparse.ArgumentParser(description='A/B Testing - Auto-Adopt Winner')
    parser.add_argument('--experiment', type=str, required=True, help='Experiment ID')
    parser.add_argument('--dry-run', action='store_true', help='Dry run mode (no changes)')
    parser.add_argument('--execute', action='store_true', help='Execute adoption')

    args = parser.parse_args()

    if not args.dry_run and not args.execute:
        print("Error: Must specify either --dry-run or --execute")
        sys.exit(1)

    # Load analysis
    analysis = load_latest_analysis(args.experiment)

    # Check if adoption is recommended
    if analysis['recommendation'] != 'auto_adopt':
        print(f"Error: Experiment '{args.experiment}' does not meet auto-adoption criteria")
        print(f"Recommendation: {analysis['recommendation']}")
        print("\nRun analysis first: python scripts/analyze-experiment.py --experiment {args.experiment}")
        sys.exit(1)

    # Determine winner
    winner_id = determine_winner(analysis)

    if not winner_id:
        print("Error: Could not determine clear winner from analysis")
        sys.exit(1)

    # Adopt winner
    adopt_winner(args.experiment, winner_id, analysis, dry_run=args.dry_run)


if __name__ == "__main__":
    main()
