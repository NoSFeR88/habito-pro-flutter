# ğŸ”¬ A/B TESTING FRAMEWORK - GUÃA COMPLETA

**VersiÃ³n**: 1.0.0
**Fecha**: 2025-10-08
**Estado**: âœ… OPERACIONAL

---

## ğŸ¯ OBJETIVO

Sistema completo de A/B testing para optimizar el proceso de desarrollo basado en datos estadÃ­sticos rigurosos, permitiendo:

- âœ… Comparar diferentes enfoques de desarrollo (templates vs manual, caching vs no-cache)
- âœ… Medir impacto real en tokens, tiempo y calidad
- âœ… Tomar decisiones basadas en evidencia estadÃ­stica
- âœ… Auto-adoptar mejores prÃ¡cticas cuando hay significancia

---

## ğŸ“Š ARQUITECTURA DEL SISTEMA

### **Componentes**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           A/B TESTING FRAMEWORK                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1. CONFIGURATION (experiments.json)                   â”‚
â”‚     - Define experiment                                â”‚
â”‚     - Specify variants                                 â”‚
â”‚     - Set metrics to track                             â”‚
â”‚                                                         â”‚
â”‚  2. EXECUTION (run-experiment.py)                      â”‚
â”‚     - Assign variant randomly                          â”‚
â”‚     - Log constraints to follow                        â”‚
â”‚     - Start execution tracking                         â”‚
â”‚                                                         â”‚
â”‚  3. COMPLETION (complete-experiment.py)                â”‚
â”‚     - Record metrics                                   â”‚
â”‚     - Save results                                     â”‚
â”‚     - Update execution log                             â”‚
â”‚                                                         â”‚
â”‚  4. ANALYSIS (analyze-experiment.py)                   â”‚
â”‚     - Statistical tests (t-test)                       â”‚
â”‚     - Effect size (Cohen's d)                          â”‚
â”‚     - Determine winner                                 â”‚
â”‚     - Generate recommendations                         â”‚
â”‚                                                         â”‚
â”‚  5. ADOPTION (auto-adopt-winner.py)                    â”‚
â”‚     - Auto-adopt if significant                        â”‚
â”‚     - Create documentation                             â”‚
â”‚     - Update defaults                                  â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ WORKFLOW PASO A PASO

### **PASO 1: Configurar Experimento**

**Archivo**: `telemetry/experiments/experiments.json`

Los experimentos estÃ¡n pre-configurados. Revisa con:

```bash
python scripts/run-experiment.py --list-experiments
```

**Ejemplo de experimento**:
```json
{
  "experiment_id": "exp-001-workflow-templates",
  "name": "Workflow Templates vs Manual Development",
  "variants": [
    {
      "id": "template",
      "name": "Using Workflow Template",
      "constraints": {
        "must_read_template": true,
        "follow_structure": true
      }
    },
    {
      "id": "manual",
      "name": "Manual Development",
      "constraints": {
        "no_template": true,
        "organic_approach": true
      }
    }
  ],
  "metrics": [
    {"name": "tokens_used", "target": "minimize"},
    {"name": "duration_minutes", "target": "minimize"},
    {"name": "quality_score", "target": "maximize"}
  ],
  "sample_size": 10
}
```

---

### **PASO 2: Ejecutar Experimento**

Cuando tengas una tarea, ejecuta:

```bash
python scripts/run-experiment.py --experiment exp-001-workflow-templates --task "Fix bug in habit provider"
```

**Output**:
```
================================================================
EXPERIMENT EXECUTION STARTED
================================================================

Execution ID: exec-20251008-230000-1234
Experiment: Workflow Templates vs Manual Development
Variant Assigned: Using Workflow Template (template)
Task: Fix bug in habit provider

================================================================
CONSTRAINTS TO FOLLOW:
================================================================

  - Must Read Template: True
  - Follow Structure: True
  - Use Optimized Bootstrap: True

================================================================
METRICS TO TRACK:
================================================================

  - tokens_used (tokens) - minimize
  - duration_minutes (minutes) - minimize
  - quality_score (0-10) - maximize

================================================================
NEXT STEPS:
================================================================

1. Execute task following variant constraints
2. Record metrics during execution
3. Run: python scripts/complete-experiment.py --execution exec-20251008-230000-1234
```

---

### **PASO 3: Ejecutar Tarea Siguiendo Constraints**

**Si variant = "template"**:
1. âœ… Leer `docs/WORKFLOW_TEMPLATES.md`
2. âœ… Seguir estructura exacta del template
3. âœ… Usar `optimized-bootstrap.ps1` para cargar contexto
4. âœ… Seguir best practices del template

**Si variant = "manual"**:
1. âœ… NO leer template
2. âœ… Desarrollar orgÃ¡nicamente segÃºn experiencia
3. âœ… Usar enfoque intuitivo

---

### **PASO 4: Completar y Registrar MÃ©tricas**

Al terminar la tarea:

```bash
python scripts/complete-experiment.py --execution exec-20251008-230000-1234 --tokens 5000 --duration 30 --quality 9 --bugs 0
```

**Output**:
```
================================================================
EXECUTION COMPLETED
================================================================

Execution ID: exec-20251008-230000-1234
Experiment: exp-001-workflow-templates
Variant: Using Workflow Template (template)

Recorded Metrics:
  - Tokens Used: 5000
  - Duration Minutes: 30
  - Quality Score: 9
  - Bugs Introduced: 0

================================================================
NEXT STEPS:
================================================================

1. Continue with more samples or
2. Run analysis: python scripts/analyze-experiment.py --experiment exp-001-workflow-templates
```

---

### **PASO 5: Analizar Resultados** (despuÃ©s de 10+ samples)

```bash
python scripts/analyze-experiment.py --experiment exp-001-workflow-templates --export-report
```

**Output**:
```
================================================================================
A/B TESTING STATISTICAL ANALYSIS
================================================================================

Experiment: Workflow Templates vs Manual Development
ID: exp-001-workflow-templates
Total Samples: 10

Variant A: template (n=5)
Variant B: manual (n=5)

================================================================================
METRIC ANALYSIS
================================================================================

Metric: tokens_used (tokens) - Target: minimize
  Variant A (template): mean=5200.00, std=450.00
  Variant B (manual): mean=12000.00, std=1200.00
  t-statistic: -5.234, p-value: 0.01
  Cohen's d: -2.150 (large)
  Winner: template (56.7% improvement)
  Significant: YES (p < 0.05)

Metric: duration_minutes (minutes) - Target: minimize
  Variant A (template): mean=28.00, std=4.00
  Variant B (manual): mean=45.00, std=8.00
  t-statistic: -4.567, p-value: 0.01
  Cohen's d: -1.890 (large)
  Winner: template (37.8% improvement)
  Significant: YES (p < 0.05)

================================================================================
RECOMMENDATION
================================================================================

AUTO-ADOPTION RECOMMENDED
  - Found significant results (p < 0.01)
  - Large effect size (d >= 0.5)
  - Sample size: 10/10

Action: Adopt winning variant as default
```

---

### **PASO 6: Auto-Adoptar Ganador** (si recomendado)

**Dry run primero**:
```bash
python scripts/auto-adopt-winner.py --experiment exp-001-workflow-templates --dry-run
```

**Ejecutar adopciÃ³n**:
```bash
python scripts/auto-adopt-winner.py --experiment exp-001-workflow-templates --execute
```

**Output**:
```
================================================================================
AUTO-ADOPTION PROCESS
================================================================================

Experiment: Workflow Templates vs Manual Development
Winner: template
Sample Size: 10

Winning Variant Details:
  Name: Using Workflow Template
  Description: Seguir template de WORKFLOW_TEMPLATES.md exactamente

Constraints to adopt:
  - Must Read Template: True
  - Follow Structure: True
  - Use Optimized Bootstrap: True

Statistical Evidence:
  - tokens_used: 56.7% improvement (p=0.01)
  - duration_minutes: 37.8% improvement (p=0.01)

================================================================================
EXECUTING ADOPTION
================================================================================

1. Documentation created: docs/adoptions/exp-001-workflow-templates_adoption.md
2. Experiment status updated to 'adopted'
3. Adoption logged to: telemetry/experiments/adoptions.json

================================================================================
ADOPTION COMPLETE
================================================================================

Next Steps:
1. Review documentation: docs/adoptions/exp-001-workflow-templates_adoption.md
2. Update workflow templates with winning approach
3. Communicate changes to team
4. Monitor continued performance
```

---

## ğŸ“‹ COMANDOS ÃšTILES

### Listar experimentos
```bash
python scripts/run-experiment.py --list-experiments
```

### Ver estado de experimento
```bash
python scripts/run-experiment.py --experiment exp-001-workflow-templates --status
```

### Listar ejecuciones pendientes
```bash
python scripts/complete-experiment.py --list-pending
```

### Analizar y exportar reporte
```bash
python scripts/analyze-experiment.py --experiment exp-001-workflow-templates --export-report
```

---

## ğŸ“Š EXPERIMENTOS PRE-CONFIGURADOS

### **1. exp-001-workflow-templates**
- **Objetivo**: Comparar templates vs desarrollo manual
- **Variantes**: template, manual
- **MÃ©tricas**: tokens, duration, quality, bugs
- **Sample size**: 10
- **Estado**: active

### **2. exp-002-context-caching**
- **Objetivo**: Comparar cache warming vs full reload
- **Variantes**: cache, no_cache
- **MÃ©tricas**: tokens, startup_time, context_quality
- **Sample size**: 10
- **Estado**: planned

### **3. exp-003-agent-specialization**
- **Objetivo**: Comparar agentes especializados vs general
- **Variantes**: specialized, general
- **MÃ©tricas**: success_rate, tokens_per_task, completion_time
- **Sample size**: 15
- **Estado**: planned

---

## ğŸ“ˆ INTERPRETACIÃ“N DE RESULTADOS

### **P-Value (Significancia)**
- **p < 0.01**: Altamente significativo â­â­â­
- **p < 0.05**: Significativo â­â­
- **p < 0.10**: Marginalmente significativo â­
- **p >= 0.10**: No significativo âŒ

### **Cohen's d (Effect Size)**
- **|d| >= 0.8**: Large effect (grande) ğŸŸ¢
- **|d| >= 0.5**: Medium effect (mediano) ğŸŸ¡
- **|d| >= 0.2**: Small effect (pequeÃ±o) ğŸŸ 
- **|d| < 0.2**: Negligible effect (negligible) âšª

### **Auto-Adoption Criteria**
Para auto-adoptar automÃ¡ticamente:
- âœ… p-value < 0.01 (highly significant)
- âœ… |Cohen's d| >= 0.5 (medium/large effect)
- âœ… Sample size alcanzado

---

## ğŸ”§ CUSTOMIZACIÃ“N

### Crear Nuevo Experimento

Editar `telemetry/experiments/experiments.json`:

```json
{
  "experiment_id": "exp-004-my-experiment",
  "name": "My Custom Experiment",
  "description": "Testing X vs Y approach",
  "status": "active",
  "variants": [
    {
      "id": "variant_a",
      "name": "Approach A",
      "constraints": { "use_x": true }
    },
    {
      "id": "variant_b",
      "name": "Approach B",
      "constraints": { "use_y": true }
    }
  ],
  "metrics": [
    {
      "name": "my_metric",
      "type": "numeric",
      "unit": "units",
      "target": "minimize"
    }
  ],
  "sample_size": 10,
  "current_samples": 0,
  "allocation": "random_50_50"
}
```

---

## âš ï¸ MEJORES PRÃCTICAS

### Durante Experimento
1. âœ… **Seguir constraints estrictamente** - La validez estadÃ­stica depende de adherencia
2. âœ… **Registrar mÃ©tricas precisas** - No estimar, usar valores reales
3. âœ… **Una tarea = una ejecuciÃ³n** - No agrupar mÃºltiples tareas
4. âœ… **Completar inmediatamente** - Registrar mÃ©tricas al terminar

### AnÃ¡lisis
1. âœ… **Esperar sample size completo** - 10+ samples por variante
2. âœ… **Verificar distribution** - Asegurar balance 50/50
3. âœ… **Considerar context** - Factores externos pueden influir
4. âœ… **Documentar assumptions** - Explicar limitaciones

### AdopciÃ³n
1. âœ… **Dry-run primero** - Verificar antes de ejecutar
2. âœ… **Comunicar cambios** - Informar al equipo
3. âœ… **Monitorear post-adoption** - Validar que mejora se mantiene
4. âœ… **Documentar learnings** - Actualizar best practices

---

## ğŸ“ ESTRUCTURA DE ARCHIVOS

```
telemetry/experiments/
â”œâ”€â”€ experiments.json              # ConfiguraciÃ³n de experimentos
â”œâ”€â”€ execution_log.json            # Log de todas las ejecuciones
â”œâ”€â”€ adoptions.json                # Log de adopciones
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ exp-001_results.json     # Resultados individuales
â”‚   â””â”€â”€ exp-002_results.json
â””â”€â”€ reports/
    â”œâ”€â”€ exp-001_analysis_20251008.json  # Reportes de anÃ¡lisis
    â””â”€â”€ exp-002_analysis_20251008.json

docs/adoptions/
â”œâ”€â”€ exp-001_adoption.md          # DocumentaciÃ³n de adopciÃ³n
â””â”€â”€ exp-002_adoption.md
```

---

## ğŸ¯ CASOS DE USO

### Caso 1: Optimizar Workflow
**Pregunta**: Â¿Templates mejoran eficiencia?
**Experimento**: exp-001-workflow-templates
**Resultado esperado**: Template reduce tokens 40-60%

### Caso 2: Validar Caching
**Pregunta**: Â¿Cache warming ahorra tokens?
**Experimento**: exp-002-context-caching
**Resultado esperado**: Cache reduce 60-70% lecturas

### Caso 3: Agent Specialization
**Pregunta**: Â¿Agentes especializados son mejores?
**Experimento**: exp-003-agent-specialization
**Resultado esperado**: Specialized agent 20-30% mÃ¡s rÃ¡pido

---

## ğŸ”® ROADMAP

### Fase Actual (Milestone 3)
- âœ… Configuration framework
- âœ… Execution tracking
- âœ… Statistical analysis
- âœ… Auto-adoption

### PrÃ³ximas Mejoras
- ğŸ”„ Multi-armed bandit allocation
- ğŸ”„ Bayesian analysis
- ğŸ”„ Continuous monitoring post-adoption
- ğŸ”„ Real-time dashboards

---

## ğŸ“ TROUBLESHOOTING

### Error: "Experiment not found"
```bash
# Verificar experimentos disponibles
python scripts/run-experiment.py --list-experiments
```

### Error: "Not enough samples"
```bash
# Verificar progreso
python scripts/run-experiment.py --experiment exp-001 --status

# Continuar ejecutando hasta sample_size
```

### Error: "No significant difference"
```bash
# Opciones:
# 1. Aumentar sample size
# 2. Refinir variantes (hacer diferencia mÃ¡s clara)
# 3. Aceptar que variantes son equivalentes
```

---

## âœ… CHECKLIST DE EXPERIMENTO

Antes de iniciar:
- [ ] Experimento configurado en `experiments.json`
- [ ] Status = "active"
- [ ] Variantes bien definidas
- [ ] MÃ©tricas claras y medibles

Durante ejecuciÃ³n:
- [ ] Seguir constraints de variante asignada
- [ ] Medir mÃ©tricas con precisiÃ³n
- [ ] Completar execution inmediatamente

Antes de analizar:
- [ ] Alcanzar sample size (10+ por variante)
- [ ] Verificar balance 50/50
- [ ] Exportar reporte

Antes de adoptar:
- [ ] p-value < 0.05 (mÃ­nimo)
- [ ] Effect size >= 0.5 (preferible)
- [ ] Dry-run primero
- [ ] Documentar decisiÃ³n

---

**FIN DE AB_TESTING_GUIDE.md**

> ğŸ“Œ **Sistema operacional**: Framework completo de A/B testing con anÃ¡lisis estadÃ­stico riguroso y auto-adopciÃ³n inteligente.
